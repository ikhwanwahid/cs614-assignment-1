{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MedQA Fine-Tuning: Teaching an LLM to Pass a Medical Exam\n",
    "\n",
    "**Task:** Fine-tune Mistral-7B-Instruct-v0.3 on USMLE-style medical multiple-choice questions using QLoRA.\n",
    "\n",
    "**Dataset:** MedQA (12,723 examples from US Medical Licensing Examination)\n",
    "\n",
    "**Metric:** Accuracy (4-option MCQ)\n",
    "\n",
    "**CS614 - Gen AI with LLMs | Individual Assignment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Google Colab Setup ────────────────────────────────────\n# Run this cell first on Colab to clone the repo and install deps.\n# Locally, this cell is a no-op.\n\nimport os\n\nIN_COLAB = \"COLAB_GPU\" in os.environ or \"COLAB_RELEASE_TAG\" in os.environ\n\nif IN_COLAB:\n    from getpass import getpass\n\n    # Clone repo (only if not already cloned)\n    if not os.path.exists(\"/content/cs614-assignment-1\"):\n        token = getpass(\"GitHub PAT: \")\n        !git clone https://{token}@github.com/ikhwanwahid/cs614-assignment-1.git /content/cs614-assignment-1\n        del token  # don't keep token in memory\n\n    # Change to project root\n    os.chdir(\"/content/cs614-assignment-1\")\n\n    # Install dependencies\n    !pip install -q transformers datasets peft bitsandbytes trl accelerate\n    !pip install -q scikit-learn matplotlib pandas tqdm\n\nimport sys\n\nPROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\")) if not IN_COLAB else os.getcwd()\nif PROJECT_ROOT not in sys.path:\n    sys.path.insert(0, PROJECT_ROOT)\n\nprint(f\"Project root: {PROJECT_ROOT}\")\nprint(f\"Running on Colab: {IN_COLAB}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "from configs.hyperparams import ExperimentConfig, get_all_configs\n",
    "from src.data_loader import (\n",
    "    format_for_inference,\n",
    "    format_for_training,\n",
    "    load_medqa_dataset,\n",
    "    prepare_datasets,\n",
    "    LABEL_MAP,\n",
    ")\n",
    "from src.model_loader import load_base_model_and_tokenizer, load_finetuned_model\n",
    "from src.trainer import create_trainer, train_and_save\n",
    "from src.evaluator import (\n",
    "    compare_models,\n",
    "    confidence_calibration,\n",
    "    error_analysis,\n",
    "    evaluate_on_dataset,\n",
    ")\n",
    "from src.baselines import run_few_shot_baseline, run_zero_shot_baseline\n",
    "from src.topic_classifier import classify_dataset\n",
    "from src.utils import (\n",
    "    plot_all_configs_comparison,\n",
    "    plot_calibration_curve,\n",
    "    plot_error_taxonomy,\n",
    "    plot_per_topic_accuracy,\n",
    "    plot_training_curves,\n",
    "    save_results_json,\n",
    "    set_seed,\n",
    "    setup_device,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load & Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw, val_raw, test_raw = load_medqa_dataset()\n",
    "\n",
    "print(f\"Train: {len(train_raw):,} examples\")\n",
    "print(f\"Val:   {len(val_raw):,} examples\")\n",
    "print(f\"Test:  {len(test_raw):,} examples\")\n",
    "print(f\"\\nColumns: {train_raw.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show example questions\n",
    "for i in range(3):\n",
    "    ex = test_raw[i]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Example {i+1} | Answer: {LABEL_MAP[ex['label']]}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(format_for_inference(ex)[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer label distribution\n",
    "train_labels = [LABEL_MAP[ex[\"label\"]] for ex in train_raw]\n",
    "label_counts = Counter(train_labels)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Label distribution\n",
    "axes[0].bar(label_counts.keys(), label_counts.values(), color=\"#4a90d9\", alpha=0.8)\n",
    "axes[0].set_title(\"Answer Label Distribution (Train)\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# Topic distribution\n",
    "test_topics = classify_dataset(test_raw)\n",
    "topic_counts = Counter(test_topics)\n",
    "topics_sorted = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "axes[1].barh([t[0] for t in topics_sorted], [t[1] for t in topics_sorted],\n",
    "             color=\"#4a90d9\", alpha=0.8)\n",
    "axes[1].set_title(\"Topic Distribution (Test Set)\")\n",
    "axes[1].set_xlabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTopics in test set:\")\n",
    "for topic, count in topics_sorted:\n",
    "    print(f\"  {topic}: {count} ({count/len(test_raw)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Zero-Shot Baseline (Base Model)\n",
    "\n",
    "Evaluate the base Mistral-7B-Instruct model with no fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config = ExperimentConfig(name=\"base_model\")\n",
    "base_model, base_tokenizer = load_base_model_and_tokenizer(base_config, for_training=False)\n",
    "# Set left padding for generation\n",
    "base_tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_results = run_zero_shot_baseline(\n",
    "    base_model, base_tokenizer, test_raw,\n",
    "    topic_labels=test_topics, batch_size=8,\n",
    ")\n",
    "\n",
    "print(f\"Zero-shot accuracy: {zero_shot_results['overall_accuracy']:.4f}\")\n",
    "print(f\"Zero-shot macro F1: {zero_shot_results['macro_f1']:.4f}\")\n",
    "print(f\"Extraction failure rate: {zero_shot_results['extraction_failure_rate']:.4f}\")\n",
    "\n",
    "save_results_json(zero_shot_results, os.path.join(PROJECT_ROOT, \"results/zero_shot_results.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Few-Shot Baseline (Base Model)\n",
    "\n",
    "Evaluate with 3 training examples as in-context exemplars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_results = run_few_shot_baseline(\n",
    "    base_model, base_tokenizer, test_raw, train_raw,\n",
    "    n_shots=3, topic_labels=test_topics, batch_size=4,\n",
    ")\n",
    "\n",
    "print(f\"3-shot accuracy: {few_shot_results['overall_accuracy']:.4f}\")\n",
    "print(f\"3-shot macro F1: {few_shot_results['macro_f1']:.4f}\")\n",
    "print(f\"Extraction failure rate: {few_shot_results['extraction_failure_rate']:.4f}\")\n",
    "\n",
    "save_results_json(few_shot_results, os.path.join(PROJECT_ROOT, \"results/few_shot_results.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free GPU memory before training\n",
    "del base_model\n",
    "torch.cuda.empty_cache()\n",
    "print(\"GPU memory freed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Sweep -- Training\n",
    "\n",
    "Train 6 configurations varying LoRA rank, learning rate, epochs, and dropout.\n",
    "\n",
    "| Config | What it tests | Key difference |\n",
    "|--------|--------------|----------------|\n",
    "| 1 (Baseline) | Standard QLoRA defaults | r=16, lr=2e-4, 2 epochs |\n",
    "| 2 (Low Rank) | Fewer params sufficient? | r=8, alpha=16 |\n",
    "| 3 (High Rank) | More capacity helps? | r=64, alpha=128, lr=1e-4 |\n",
    "| 4 (Low LR) | Slower convergence? | lr=5e-5, 3 epochs |\n",
    "| 5 (Extended) | Optimal stopping point | 3 epochs, lr=2e-4 |\n",
    "| 6 (Aggressive) | Speed + regularization | r=32, lr=3e-4, dropout=0.1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_all_configs()\n",
    "all_training_logs = {}\n",
    "all_val_results = {}\n",
    "\n",
    "for config_name, config in configs.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {config_name}\")\n",
    "    print(f\"Description: {config.description}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    set_seed(config.seed)\n",
    "\n",
    "    # Prepare data\n",
    "    train_ds, val_ds, _ = prepare_datasets(config)\n",
    "\n",
    "    # Load model with LoRA\n",
    "    model, tokenizer = load_base_model_and_tokenizer(config, for_training=True)\n",
    "\n",
    "    # Train\n",
    "    trainer = create_trainer(model, tokenizer, train_ds, val_ds, config)\n",
    "    training_metrics = train_and_save(trainer, config)\n",
    "    all_training_logs[config_name] = training_metrics[\"log_history\"]\n",
    "\n",
    "    # Quick validation accuracy\n",
    "    tokenizer.padding_side = \"left\"  # switch for generation\n",
    "    val_results = evaluate_on_dataset(model, tokenizer, val_raw, batch_size=8)\n",
    "    all_val_results[config_name] = val_results\n",
    "    print(f\"  Val accuracy: {val_results['overall_accuracy']:.4f}\")\n",
    "\n",
    "    # Free memory\n",
    "    del model, trainer\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"All training complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual training curves\n",
    "for config_name, log_history in all_training_logs.items():\n",
    "    plot_training_curves(\n",
    "        log_history, title=f\"Training Curves: {config_name}\",\n",
    "        save_path=os.path.join(PROJECT_ROOT, f\"results/{config_name}/training_curve.png\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All configs overlaid\n",
    "plot_all_configs_comparison(\n",
    "    all_training_logs,\n",
    "    save_path=os.path.join(PROJECT_ROOT, \"results/all_configs_comparison.png\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation accuracy summary table\n",
    "val_summary = []\n",
    "for name, res in all_val_results.items():\n",
    "    val_summary.append({\n",
    "        \"Config\": name,\n",
    "        \"Val Accuracy\": f\"{res['overall_accuracy']:.4f}\",\n",
    "        \"Val Macro F1\": f\"{res['macro_f1']:.4f}\",\n",
    "        \"Extraction Failures\": f\"{res['extraction_failure_rate']:.4f}\",\n",
    "    })\n",
    "\n",
    "val_df = pd.DataFrame(val_summary).sort_values(\"Val Accuracy\", ascending=False)\n",
    "print(val_df.to_string(index=False))\n",
    "\n",
    "best_config_name = val_df.iloc[0][\"Config\"]\n",
    "print(f\"\\nBest config: {best_config_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Config -- Full Test Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best fine-tuned model\n",
    "best_config = configs[best_config_name]\n",
    "adapter_path = os.path.join(PROJECT_ROOT, f\"results/{best_config_name}/adapter\")\n",
    "\n",
    "ft_model, ft_tokenizer = load_finetuned_model(best_config, adapter_path)\n",
    "print(f\"Loaded adapter from: {adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full test set evaluation\n",
    "ft_results = evaluate_on_dataset(\n",
    "    ft_model, ft_tokenizer, test_raw,\n",
    "    topic_labels=test_topics, batch_size=8,\n",
    ")\n",
    "\n",
    "print(f\"Fine-tuned accuracy: {ft_results['overall_accuracy']:.4f}\")\n",
    "print(f\"Fine-tuned macro F1: {ft_results['macro_f1']:.4f}\")\n",
    "print(f\"Extraction failures: {ft_results['extraction_failure_rate']:.4f}\")\n",
    "print(f\"\\nClassification Report:\\n{ft_results['classification_report']}\")\n",
    "\n",
    "save_results_json(ft_results, os.path.join(PROJECT_ROOT, \"results/finetuned_test_results.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Comparison: Zero-Shot vs Few-Shot vs Fine-Tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison = compare_models(zero_shot_results, ft_results, few_shot_results)\n",
    "\n",
    "comp_df = pd.DataFrame({\n",
    "    \"Metric\": [\"Accuracy\", \"Macro F1\", \"Extraction Failure Rate\"],\n",
    "    \"Zero-Shot\": [\n",
    "        comparison[\"zero_shot\"][\"accuracy\"],\n",
    "        comparison[\"zero_shot\"][\"macro_f1\"],\n",
    "        comparison[\"zero_shot\"][\"extraction_failures\"],\n",
    "    ],\n",
    "    \"3-Shot\": [\n",
    "        comparison[\"few_shot\"][\"accuracy\"],\n",
    "        comparison[\"few_shot\"][\"macro_f1\"],\n",
    "        comparison[\"few_shot\"][\"extraction_failures\"],\n",
    "    ],\n",
    "    \"Fine-Tuned\": [\n",
    "        comparison[\"fine_tuned\"][\"accuracy\"],\n",
    "        comparison[\"fine_tuned\"][\"macro_f1\"],\n",
    "        comparison[\"fine_tuned\"][\"extraction_failures\"],\n",
    "    ],\n",
    "})\n",
    "\n",
    "print(comp_df.to_string(index=False))\n",
    "print(f\"\\nAccuracy improvement (fine-tuned vs zero-shot): +{comparison['delta']['accuracy']:.4f}\")\n",
    "\n",
    "save_results_json(comparison, os.path.join(PROJECT_ROOT, \"results/comparison_summary.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Per-Topic Accuracy Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_topic_accuracy(\n",
    "    ft_results[\"per_topic_accuracy\"],\n",
    "    base_per_topic=zero_shot_results.get(\"per_topic_accuracy\"),\n",
    "    title=\"Per-Topic Accuracy: Base vs Fine-Tuned\",\n",
    "    save_path=os.path.join(PROJECT_ROOT, \"results/per_topic_accuracy.png\"),\n",
    ")\n",
    "\n",
    "# Print per-topic table\n",
    "topic_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Topic\": topic,\n",
    "        \"Base Accuracy\": f\"{zero_shot_results.get('per_topic_accuracy', {}).get(topic, 0):.3f}\",\n",
    "        \"Fine-Tuned Accuracy\": f\"{acc:.3f}\",\n",
    "        \"Delta\": f\"{acc - zero_shot_results.get('per_topic_accuracy', {}).get(topic, 0):+.3f}\",\n",
    "    }\n",
    "    for topic, acc in sorted(ft_results[\"per_topic_accuracy\"].items(),\n",
    "                             key=lambda x: x[1], reverse=True)\n",
    "])\n",
    "print(topic_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = error_analysis(ft_results[\"predictions\"])\n",
    "\n",
    "print(f\"Total errors: {errors['total_errors']} / {ft_results['n_total']}\")\n",
    "print(f\"\\nError breakdown:\")\n",
    "for cat, count in errors[\"error_counts\"].items():\n",
    "    print(f\"  {cat}: {count}\")\n",
    "\n",
    "print(f\"\\nMost confused answer pairs (gold -> predicted):\")\n",
    "for (gold, pred), count in errors[\"most_confused_pairs\"][:5]:\n",
    "    print(f\"  {gold} -> {pred}: {count} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_taxonomy(\n",
    "    errors[\"error_counts\"],\n",
    "    save_path=os.path.join(PROJECT_ROOT, \"results/error_taxonomy.png\"),\n",
    ")\n",
    "\n",
    "# Show example errors\n",
    "print(\"\\nExample substantive errors:\")\n",
    "for ex in errors[\"error_examples\"][\"substantive_error\"][:3]:\n",
    "    print(f\"\\n  Q index: {ex['idx']}\")\n",
    "    print(f\"  Gold: {ex['gold']} | Predicted: {ex['pred']}\")\n",
    "    print(f\"  Topic: {ex.get('topic', 'N/A')}\")\n",
    "    print(f\"  Raw output: {ex['raw_output'][:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate by topic\n",
    "topic_err_df = pd.DataFrame([\n",
    "    {\"Topic\": topic, \"Error Rate\": f\"{rate:.3f}\"}\n",
    "    for topic, rate in sorted(errors[\"topic_error_rates\"].items(),\n",
    "                               key=lambda x: x[1], reverse=True)\n",
    "])\n",
    "print(\"Error rate by topic:\")\n",
    "print(topic_err_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Confidence Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_data = confidence_calibration(\n",
    "    ft_model, ft_tokenizer, test_raw,\n",
    "    topic_labels=test_topics, batch_size=8,\n",
    ")\n",
    "\n",
    "print(f\"Expected Calibration Error (ECE): {cal_data['ece']:.4f}\")\n",
    "print(f\"Avg confidence (correct):   {cal_data['avg_confidence_correct']:.4f}\")\n",
    "print(f\"Avg confidence (incorrect): {cal_data['avg_confidence_incorrect']:.4f}\")\n",
    "\n",
    "plot_calibration_curve(\n",
    "    cal_data,\n",
    "    save_path=os.path.join(PROJECT_ROOT, \"results/calibration_curve.png\"),\n",
    ")\n",
    "\n",
    "save_results_json(cal_data, os.path.join(PROJECT_ROOT, \"results/calibration_data.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "del ft_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary & Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "*(Fill in after running experiments)*\n",
    "\n",
    "1. **Fine-tuning improved accuracy from X% (zero-shot) to Y% (+Z points)**\n",
    "2. **Best hyperparameter configuration:** ...\n",
    "3. **Strongest topics:** ...\n",
    "4. **Weakest topics / failure modes:** ...\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. **Heuristic topic classifier** -- keyword-based, may misclassify multi-topic questions\n",
    "2. **Single model family** -- only tested Mistral-7B; results may differ on Llama-3 or Phi-3\n",
    "3. **No chain-of-thought** -- model outputs a single letter without reasoning; CoT training could improve accuracy\n",
    "4. **Dataset scope** -- MedQA covers USMLE only (US-centric medical practice)\n",
    "\n",
    "### Ethical Considerations\n",
    "\n",
    "1. **Clinical risk** -- This model must NOT be used for actual medical decisions. Even at 65-70% accuracy, 1 in 3 answers is wrong.\n",
    "2. **Cultural bias** -- USMLE reflects US medical practice, guidelines, and drug formularies. Performance would likely degrade on non-US medical contexts.\n",
    "3. **Overconfidence** -- Fine-tuned LLMs can be confidently wrong. The calibration analysis above quantifies this risk.\n",
    "4. **Demographic blind spots** -- Clinical vignettes in MedQA may underrepresent certain demographics, leading to performance disparities.\n",
    "\n",
    "### Alternative Design Choices\n",
    "\n",
    "- **Prompt engineering (Medprompt)** -- Microsoft showed GPT-4 with careful prompting can reach 90%+ without fine-tuning\n",
    "- **RAG** -- Retrieval-augmented generation using medical textbooks could ground the model's knowledge\n",
    "- **Larger model** -- Fine-tuning a 70B model would likely yield higher accuracy but requires more compute\n",
    "- **MedMCQA augmentation** -- Adding 183K Indian medical exam questions with explanations for CoT training\n",
    "\n",
    "### Reproducibility\n",
    "\n",
    "- **Model:** `mistralai/Mistral-7B-Instruct-v0.3`\n",
    "- **Dataset:** `GBaker/MedQA-USMLE-4-options-hf`\n",
    "- **Libraries:** See `pyproject.toml` for exact versions\n",
    "- **Seed:** 42\n",
    "- **Compute:** Google Colab Pro, NVIDIA A100 40GB\n",
    "- **All code:** Available in `src/` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}